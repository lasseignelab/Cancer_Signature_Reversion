---
title: "220124_tau_values_LINCS"
author: "Jennifer Fisher"
date: "1/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
meta42 <- readr::read_tsv("/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/siginfo_beta.txt")
dose <- "10 uM"
## filter rows by 'pert_type' as compound, 10uM concentration, and 24h treatment time
#meta42_filter <- sig_filter(meta42, pert_type="trt_cp", dose=dose, time="24 h") 
```

```{r}
sig_filter_JLF<- function (meta, pert_type = "trt_cp", dose, time = "24 h") {
    meta %<>% dplyr::filter(pert_type == pert_type & pert_idose == 
        dose & pert_itime == time)
    meta %<>% bind_cols(alt_id = paste(meta$cmap_name, meta$cell_iname, 
        sep = "_")) %>% bind_cols(pert_cell_factor = paste(meta$cmap_name, 
        meta$cell_iname, meta$pert_type, sep = "__")) %>% distinct(alt_id, 
        .keep_all = TRUE)
    return(meta)
}
```

```{r}
library(magrittr)
library(dplyr)
meta42_filter <- sig_filter_JLF(meta42, pert_type="trt_cp", dose=dose, time="24 h") 
```




```{r}
#########################
## make taurefList.rds ##
#########################

# It uses all signatures in the reference database (such as LINCS) to query 
# against itself as Qref to compute tau score of `gess_lincs` 
# method in `signatureSearch` package. Tau score compares observed enrichment 
# score to all others in Qref. It represents the percentage of reference queries
# with a lower |NCS| than |NCSq,r|, adjusted to retain the sign of NCSq,r. 
# NCSq,r is the normalized connectivity score for signature r relative to 
# query q. A tau of 90 indicates that only 10 percent of reference perturbations
# showed stronger connectivity to the query. For more details, please refer to 
# Subramanian et al., 2017, Cell, A Next Generation Connectivity Map: L1000 
# Platform and the First 1,000,000 Profiles

## Create Query Reference DB for Tau Score Computation of `gess_lincs` method
### Load `lincs` database created above
library(HDF5Array); library(SummarizedExperiment)
#se = loadHDF5SummarizedExperiment("/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
se<- SummarizedExperiment(HDF5Array("/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5",name= "assay"))


#missing colData
#se@colData<- as.matrix(meta42_filter)
se <- SummarizedExperiment(assays=list(assay=score_mat), colData=meta42_filter)
rownames(se) <- HDF5Array("/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", name="rownames")
colnames(se) <- HDF5Array("/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", name="colnames")
score_mat = assay(se)
### Create query list for all signatures in se
query_list <- lapply(colnames(score_mat), function(x) {
  vec <- as.matrix(score_mat[,x])
  names(vec) <- rownames(score_mat)
  sigvec = sort(vec, decreasing = TRUE)
  list(upset=utils::head(names(sigvec), 150), 
       downset=utils::tail(names(sigvec), 150))
})
names(query_list) = colData(se)$pert_cell_factor
```

```{r}
###  Query signatures in the query_list against lincs database
###  To save time, the processing is paralleled with BiocParallel to run on 
###  CPU cores of a computer cluster with a scheduler (e.g. Slurm). 
#### Define submission function
f <- function(x, se, query_list, dest_dir) {
  require(signatureSearch)
  chunkno <- x 
  sz <- 10 # small enough to use short queue 
  qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz))
  myMA <- matrix(NA, length(query_list), sz, 
                 dimnames=list(names(query_list), seq_len(sz)))
  qlistone <- qlist[[chunkno]] 
  #return(qlistone)
  #seq_along(qlistone)
   for(i in seq_along(qlistone)){
     qsig <- suppressMessages(qSig(query=list(upset=qlistone[[i]]$upset, 
                                             downset=qlistone[[i]]$downset), 
                  gess_method = "LINCS", refdb = se))
     lincs <- signatureSearch::gess_lincs(qsig, sortby=NA, workers= 10)
     resultDF <- result(lincs)
     ncs <- resultDF$NCS
     mynames <- paste(resultDF$pert, resultDF$cell, resultDF$type, sep="__")
     names(ncs) <- mynames
     myMA[,i] <- ncs[rownames(myMA)]
     colnames(myMA)[i] <- names(qlistone[i])
   }
   myMA <- myMA[, seq_along(qlistone)] 
   ## Only relevant for last entry that may not have as many columns as sz
   if(! dir.exists(dest_dir)) dir.create(dest_dir)
   write.table(as.data.frame(myMA), 
               file=paste0(dest_dir, "/result_", sprintf("%03d", chunkno)), 
               col.names=NA, quote=FALSE, sep="\t")
}
```

```{r}
library(parallel)
```


```{r}
date()
mclapply(1:2, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
date()
```
```{r}
#3-8 done 
date()
mclapply(9:1000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
date()
mclapply(1001:2000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
mclapply(2001:3000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
mclapply(3001:4000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
mclapply(4001:5000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
```


```{r}
mclapply(5001:6000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
mclapply(6001:7000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
mclapply(8001:9000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
mclapply(9001:10000, f, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020", mc.cores= 8)
```




```{r}
date()
f(1, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
date()
f(2, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(3, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(4, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(5, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(6, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(7, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(8, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(9, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(10, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
f(11, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020/lincs_2020.h5", query_list, "/data/project/lasseigne_lab/JLF_scratch/Transfer_Learning_R03/data/LINCS_2020")
```



```{r}
#### Split query into small chunks, each chunk will be processed on computer 
#### cluster as one process. Use `batchtools` to manage job submission.
#### Please make sure that `data` directory exists under your current working
#### directory of R session
library(batchtools)
sz <- 10 # small enough to finish each chunk within 2 hours
qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz))
if(! file.exists("slurm.tmpl")) 
  download.file("https://goo.gl/tLMddb", "slurm.tmpl", quiet=TRUE)
if(! file.exists(".batchtools.conf.R")) 
  download.file("https://goo.gl/5HrYkE", ".batchtools.conf.R", quiet=TRUE)
if(dir.exists("data/tau_queries_reg")) 
  unlink("data/tau_queries_reg", recursive=TRUE)
reg = makeRegistry(file.dir="data/tau_queries_reg", 
                   conf.file=".batchtools.conf.R")
dest_dir="data/tau_queries" 
## directory to store the intermediate results generated by function f
ids = batchMap(f, x = seq(along=qlist), 
            more.args = list(se=se, query_list=query_list, dest_dir=dest_dir))
#testJob(id = 1)
done <- submitJobs(ids, resources=list(walltime=36000, ncpus=1, memory=10240), 
                   partition="short", reg=reg)
#waitForJobs()
getStatus()

#### Inspect result and resubmit jobs for missing and empty files
fileDF <- file.info(list.files(dest_dir, pattern="result_*", full.names=TRUE))
index_empty <- as.numeric(gsub(".*_", "", row.names(fileDF[fileDF$size==0, ])))
qlist <- split(query_list, ceiling(seq_along(names(query_list))/sz))
index_all_files <- seq_along(qlist)
index_exist <- as.numeric(gsub(".*_", "", row.names(fileDF)))
index_missing <- index_all_files[!index_all_files %in% index_exist]
index_repeat <- unique(sort(c(index_empty, index_missing)))
if(length(index_repeat)!=0){
  if(dir.exists("data/tau_queries_reg")) 
    unlink("data/tau_queries_reg", recursive=TRUE)
  reg = makeRegistry(file.dir="data/tau_queries_reg", 
                     conf.file=".batchtools.conf.R")
  ids = batchMap(f, x = index_repeat, more.args = 
                   list(se=se, query_list=query_list, dest_dir=dest_dir))
  done <- submitJobs(ids, resources=list(walltime=36000, ncpus=1, 
                                    memory=10240, partition="short"), reg=reg)
}
## Repeat the above steps until all jobs are successfully completed 
## (index_repeat is empty)

```


```{r}


#### Organize results in list where each component contains data.frame with 
#### query results from one cell type
pathDF <- data.frame(query=names(query_list), 
                     path=rep(paste0("data/tau_queries/result_", 
                                     sprintf("%03d", seq_along(qlist))), 
                              sapply(qlist, length)))
pathDF <- data.frame(pathDF, target=gsub("^.*?__", "", pathDF$query))
pathList <- split(as.character(pathDF$path), factor(pathDF$target))
pathList <- sapply(pathList, unique) # eliminates duplicated imports of files
taurefList <- rep(NA, length(pathList)); names(taurefList) <- names(pathList)
taurefList <- as.list(taurefList) 
for(celltype in names(pathList)) {
  for(i in seq_along(pathList[[celltype]])) {
    tmpDF <- read.delim(pathList[[celltype]][i], row.names=1, check.names=FALSE)
    tmpDF <- round(tmpDF, 2) # Reduces data size
    colindex <- gsub("^.*?__", "", colnames(tmpDF)) %in% celltype
    tmpDF <- tmpDF[, colindex, drop=FALSE] 
    if(i==1) { 
      rowindex <- gsub("^.*?__", "", rownames(tmpDF)) %in% celltype
      containerDF <- tmpDF[rowindex, , drop=FALSE]
    } else {
      containerDF <- cbind(containerDF, tmpDF[rownames(containerDF),])
    }
    print(paste("Finished", i, "of", length(pathList[[celltype]]), 
                "results collected in", ncol(containerDF), "columns."))
  }
  taurefList[[celltype]] <- containerDF
  rm(containerDF); gc()
}
saveRDS(taurefList, file="data/tau_queries/taurefList.rds")

```

